import fs::*;
import fmt::*;

enum TokenType {
  Invalid,
  True,
  False,
  Null,
  Eof,
  Colon,
  Number,
  String,
  LCurly,
  RCurly,
  LBracket,
  RBracket,
  Comma,
}

struct Token {
  type: TokenType,
  value: String,
  line: s32,
  column: s32,
}

impl Token {
  fn Invalid() -> #self {
    v: #self;
    return v;
  }
}

struct JsonLexer {
  line_start: s32,
  cur_line: s32,
  // filename/source of input.
  file: String,
  // input data.
  input: String,
  // position in input stream.
  position: u32,
  // lookahead buffer
  lookahead_buffer : List!<Token>,
}

impl JsonLexer {
  fn create!<TString>(file: TString, input: String) -> #self where TString: AsByteSlice {
    return .{
      file: String::from(file.as_byte_slice()),
      input: input,
    };
  }
  
  fn get_location_string(self) -> String {
    column := self.position - self.line_start;
    line := self.cur_line + 1;
    return format("%:%:%", (self.file, line, column), FormatOptions::default());
  }

  fn get_token(*mut self) -> Result!<Token, String> {
    alias Result :: Result!<Token, String>;
    mut token : Token;

    token.type = TokenType::Eof;
    token.column = self.position - self.line_start;
    token.line = self.cur_line + 1;

    if self.position >= self.input.length || self.input[self.position] == 0 {
      return Result::Ok(token);
    }


    // TODO: handle UTF8.
    mut c := self.input[self.position];

    // whitespace
    // TODO: this should handle tabs, no? perhaps isspace does that.
    while self.position < self.input.length {
      if c == '\n' || c == '\r' {
        self.line_start = self.position;
        self.cur_line++;
      } else if !std::c::isspace(c) {
        break;
      }
      self.position++;
      c = self.input[self.position];
    }

    token.column = self.position - self.line_start;
    token.line = self.cur_line + 1;

    if self.position >= self.input.length || self.input[self.position] == 0 {
      return Result::Ok(token);
    }

    // comments
    if c == '/' {
      self.position++;
      if self.position >= self.input.length {
        mut location := self.get_location_string();
        defer location.destroy();
        return Result::Err(format("at \"%\" unexpected end of input %", (location, c), FormatOptions::default(),))
      }
      c = self.input[self.position];
      if c != '/' {
        location := self.get_location_string();
        return Result::Err(format("at \"%\" expected /, got %", (location, c), FormatOptions::default(),))
      }
      while c != '\n' && c != '\r' {
        self.position++;
        if self.position >= self.input.length || self.input[self.position] == 0 {
          token.type = TokenType::Eof;
          return Result::Ok(token);
        }
        c = self.input[self.position];
      }
      return self.get_token();
    }


    if c.is_digit() || c == '-' {
      // numbers
      len : s32;
      start : s32 = self.position;

      mut is_negative: bool;
      if c == '-' {
        self.position++;
        is_negative = true;
        c = self.input[self.position];
      }

      while (c >= '0' && c <= '9' || c == '.') && self.position < self.input.length {
        len++;
        self.position++;
        c = self.input[self.position];
      }

      token.value = .{
        data: std::c::strndup(self.input.data + start, len),
        length: len
      };

      // Since we don't have operators we just handle the only case, negative numbers, here.
      if is_negative {
        token.value.push_front('-');
      }

      token.type = TokenType::Number;
      return Result::Ok(token);
      // operators
    } else if c == ':' {
      token.value = String::from(":");
      token.type = TokenType::Colon;
      self.position++;
      return Result::Ok(token);
    } else if c == '{' {
      token.value = String::from("{");
      token.type = TokenType::LCurly;
      self.position++;
      return Result::Ok(token);
    } else if c == '}' {
      token.value = String::from("}");
      token.type = TokenType::RCurly;
      self.position++;
      return Result::Ok(token);
    } else if c == '[' {
      token.value = String::from("[");
      token.type = TokenType::LBracket;
      self.position++;
      return Result::Ok(token);
    } else if c == ']' {
      token.value = String::from("]");
      token.type = TokenType::RBracket;
      self.position++;
      return Result::Ok(token);
    } else if c == ',' {
      token.value = String::from(",");
      token.type = TokenType::Comma;
      self.position++;
      return Result::Ok(token);
    } else if c == '\"' {
      self.position++;
      c = self.input[self.position];
      len : s32;
      start : s32 = self.position;
      while c != '\"' && self.position < self.input.length {
        if c == '\n' || c == '\r' {
          self.line_start = self.position;
          self.cur_line++;
        }
        len++;
        self.position++;
        c = self.input[self.position];
      }
      self.position++;

      token.value = .{
        data: std::c::strndup(self.input.data + start, len),
        length: len
      };

      token.type = TokenType::String;
      return Result::Ok(token);
    } else if self.position + 4 <= self.input.length && std::c::strncmp(self.input.data + self.position, "true"c, 4) == 0 {
      token.value = String::from("true"); 
      token.type = TokenType::True;
      self.position += 4;
      return Result::Ok(token);
    } else if self.position + 5 <= self.input.length && std::c::strncmp(self.input.data + self.position, "false"c, 5) == 0 {
      token.value = String::from("false");
      token.type = TokenType::False;
      self.position += 5;
      return Result::Ok(token);
    } else if self.position + 4 <= self.input.length && std::c::strncmp(self.input.data + self.position, "null"c, 4) == 0 {
      token.value = String::from("null");
      token.type = TokenType::Null;
      self.position += 4;
      return Result::Ok(token);
    } else {
      return Result::Ok(token);
    }
  }
}

/* 
TODO: 
  we could use the 'elexify' library, it's a port of the lexer that parses Ela itself.
  Should be pretty reliable and thorough.
*/

impl JsonLexer {

  fn ensure_buffer_is_full(*mut self) {
    while self.lookahead_buffer.length < 8 {
      self.lookahead_buffer.push(self.get_token().unwrap());
    }
  }

  fn eat(*mut self) -> Token {
    self.ensure_buffer_is_full();
    token := self.lookahead_buffer[0];
    for i in 0..self.lookahead_buffer.length - 1 {
      self.lookahead_buffer.data[i] = self.lookahead_buffer.data[i + 1];
    }
    self.lookahead_buffer.length--;
    return token;
  }

  fn peek(*mut self) -> Token {
    self.ensure_buffer_is_full();
    return self.lookahead_buffer[0];
  }

  fn expect_types(*mut self, types: InitList!<TokenType>) -> Result!<Token, String> {
    token := self.eat();

    // this should never happen.
    // we are the consumer of this API, but I guess maybe someone might want to use this?

    // Edit: no one should use this. they should use 'elexify' if they need a lexer.
    if types.length == 0 {
      loc := #location;
      panic(format("json lexers `expect_types` got an empty set of types to expect, at \"%\"", (loc,), FormatOptions::default(),).as_str());
    }

    for i in 0..types.length {
      type := types.data[i];
      if token.type == type {
        return Result!<Token, String>::Ok(token);
      }
    }
  
    error := format("at: \"%:%:%\" :: unexpected token, expected %, got %", (
        self.file, token.line, token.column,
        types,
        token.type,
      ),
      *FormatOptions::current(),
    );

    return Result!<Token, String>::Err(error);
  }

  fn expect_type(*mut self, type: TokenType) -> Result!<Token, String> {
    // TODO: it's much cheaper if we just check this, instead of making list lol.
    // saves a stack alloc and a function call.
    return self.expect_types(.[type]);
  }
}